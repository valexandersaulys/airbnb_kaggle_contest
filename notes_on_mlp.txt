Notes
------

I looked at MLPs with Keras. I found that, using the protoAlpha_ data, the neural
networks were not training

1 layer, 64 neurons, lr0.1, sigmoid
Using gpu device 0: GeForce GTX 980
Epoch 1/25
119867/119867 [==============================] - 276s - loss: 2.0178       
Epoch 2/25
119867/119867 [==============================] - 284s - loss: 2.0178      
Epoch 3/25
119867/119867 [==============================] - 283s - loss: 2.0178

1 layer, 1024 neurons, lr0.1, sigmoid
Using gpu device 0: GeForce GTX 980
Epoch 1/25
119867/119867 [==============================] - 271s - loss: 6.7821       
Epoch 2/25
119867/119867 [==============================] - 274s - loss: 6.7821      
Epoch 3/25
119867/119867 [==============================] - 272s - loss: 6.7821

1 layer, 1024 neurons, lr0.01, sigmoid
Epoch 1/25
119867/119867 [==============================] - 267s - loss: 2.0115      
Epoch 2/25
119867/119867 [==============================] - 273s - loss: 2.0113      
Epoch 3/25
119867/119867 [==============================] - 271s - loss: 2.0113      
Epoch 4/25
119867/119867 [==============================] - 276s - loss: 2.0113

1 layer, 1024 neurons, lr0.001, sigmoid
Epoch 1/25
119867/119867 [==============================] - 276s - loss: 1.9111       
Epoch 2/25
119867/119867 [==============================] - 280s - loss: 2.0068      
Epoch 3/25
119867/119867 [==============================] - 279s - loss: 2.0068

1 layer, 4096 neurons, lr0.1, sigmoid
Epoch 1/25
119867/119867 [==============================] - 282s - loss: 15.7259      
Epoch 2/25
119867/119867 [==============================] - 290s - loss: 15.7259      
Epoch 3/25
 23929/119867 [====>.........................] - ETA: 192s - loss: 15.7220

1 layer, 4096 neurons, lr0.001, tanh
Using gpu device 0: GeForce GTX 980
Epoch 1/25
119867/119867 [==============================] - 274s - loss: 2.0171       
Epoch 2/25
119867/119867 [==============================] - 290s - loss: 2.0170      
Epoch 3/25
119867/119867 [==============================] - 284s - loss: 2.0170
Epoch 4/25
 49264/119867 [===========>..................] - ETA: 168s - loss: 2.0004


1 layer, 4096 neurons, adagrad(default), tanh
Epoch 1/25
119867/119867 [==============================] - 285s - loss: 2.0067       
Epoch 2/25
119867/119867 [==============================] - 285s - loss: 2.0066      
Epoch 3/25
119867/119867 [==============================] - 281s - loss: 2.0066       
Epoch 4/25
119867/119867 [==============================] - 279s - loss: 2.0066      
Epoch 5/25
119867/119867 [==============================] - 280s - loss: 2.0066      
Epoch 6/25
119867/119867 [==============================] - 277s - loss: 2.0066       
Epoch 7/25
 75744/119867 [=================>............] - ETA: 100s - loss: 1.9920



2 layers, 2x4096 neurons, adagrad(default), sigmoid
Epoch 1/25
119867/119867 [==============================] - 1094s - loss: 6.7769      
Epoch 2/25
119867/119867 [==============================] - 1094s - loss: 6.7768      
Epoch 3/25
  1397/119867 [..............................] - ETA: 1087s - loss: 6.5534


1 layer, 12000 neurons, adagrad(default), sigmoid
Epoch 1/25
119867/119867 [==============================] - 282s - loss: 15.9059      
Epoch 2/25
119867/119867 [==============================] - 285s - loss: 15.9059      
Epoch 3/25
119867/119867 [==============================] - 276s - loss: 15.9059 


1 layer, 12000 neurons, adagrad(default), sigmoid, batches of 128
sucked
Epoch 23/25
119867/119867 [==============================] - 3s - loss: 2.0068      
Epoch 24/25
119867/119867 [==============================] - 3s - loss: 2.0068      
Epoch 25/25
119867/119867 [==============================] - 3s - loss: 2.0068


1 layer, 12000 neurons, sgd(lr=0.0001), sigmoid, batches of 128

